{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document_Classification_IDT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ZfFBs-hMkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATPNbrhbp98N",
        "colab_type": "text"
      },
      "source": [
        "### Load Data\n",
        "Download data from 20 news groups "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku40bp3hheea",
        "colab_type": "code",
        "outputId": "32204908-71fc-460d-c96b-c11f9a286cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "print(list(newsgroups_train.target_names))\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qSs128xqC7g",
        "colab_type": "text"
      },
      "source": [
        "##Prepare the Data\n",
        "To keep it simple, let's filter only 5 of the 20 topics. \n",
        "We will then convert the unstructured text to a structured vector of thousands of features made up of the words from the documents.  Stop words like “is”, “the”, “it” wil be removed.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOE5Zcf5hlSW",
        "colab_type": "code",
        "outputId": "134e4371-1b26-4c62-d8eb-23e31df7cc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#Categories     0               1                   2               3             4\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'sci.space', 'talk.politics.guns']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, \n",
        "                                      shuffle=True, random_state=2017, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, \n",
        "                                     shuffle=True, random_state=2017, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Convert a collection of raw documents to a matrix of TF-IDF features\n",
        "vectorizer = TfidfVectorizer()    # This one is the basic text to feature vector function, try some of the options\n",
        "vectorizer = TfidfVectorizer(lowercase=False, stop_words='english')\n",
        "vectorizer = TfidfVectorizer(smooth_idf = True, max_df=0.5, stop_words='english')\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df=0.5,  ngram_range=(1, 2), stop_words='english')\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)  # Learn vocabulary and idf, return term-document matrix.\n",
        "X_test = vectorizer.transform(newsgroups_test.data)        # Transform documents to term-document matrix.\n",
        "\n",
        "print(\"Train Dataset\")\n",
        "print(\"%d documents\" % len(newsgroups_train.data))\n",
        "print(\"%d categories\" % len(newsgroups_train.target_names))\n",
        "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
        "\n",
        "print(\"\\nTest Dataset\")\n",
        "print(\"%d documents\" % len(newsgroups_test.data))\n",
        "print(\"%d categories\" % len(newsgroups_test.target_names))\n",
        "print(\"n_samples: %d, n_features: %d\" % X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset\n",
            "2801 documents\n",
            "5 categories\n",
            "n_samples: 2801, n_features: 241036\n",
            "\n",
            "Test Dataset\n",
            "1864 documents\n",
            "5 categories\n",
            "n_samples: 1864, n_features: 241036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy6V3JSwqFoz",
        "colab_type": "text"
      },
      "source": [
        "### Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NN_zuuIhbTx",
        "colab_type": "code",
        "outputId": "5ae03f65-5389-405c-95c0-30c0192421b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion = 'entropy', random_state=0, min_samples_leaf = 3, max_depth=None, min_samples_split=2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# generate evaluation metrics\n",
        "print(\"Categories: 0=alt.atheism, 1=comp.graphics, 2=rec.motorcycles, 3=sci.space, 4=talk.politics.guns\\n\")\n",
        "print (\"Train - Accuracy :\", metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
        "print (\"Train - Confusion matrix :\\n\", metrics.confusion_matrix(y_train, clf.predict(X_train)))\n",
        "print (\"Train - classification report :\\n\", metrics.classification_report(y_train, clf.predict(X_train)))\n",
        "\n",
        "print (\"Test - Accuracy :\", metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
        "print (\"Test - Confusion matrix :\\n\",metrics.confusion_matrix(y_test, clf.predict(X_test)))\n",
        "print (\"Test - classification report :\\n\", metrics.classification_report(y_test, clf.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: 0=alt.atheism, 1=comp.graphics, 2=rec.motorcycles, 3=sci.space, 4=talk.politics.guns\n",
            "\n",
            "Train - Accuracy : 0.8418367346938775\n",
            "Train - Confusion matrix :\n",
            " [[279   4  30  12   7]\n",
            " [ 13 349  31  16   5]\n",
            " [ 14  17 370   7   7]\n",
            " [ 15  16  37 330  10]\n",
            " [ 17   8  36   8 322]]\n",
            "Train - classification report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83       332\n",
            "           1       0.89      0.84      0.86       414\n",
            "           2       0.73      0.89      0.81       415\n",
            "           3       0.88      0.81      0.85       408\n",
            "           4       0.92      0.82      0.87       391\n",
            "\n",
            "    accuracy                           0.84      1960\n",
            "   macro avg       0.85      0.84      0.84      1960\n",
            "weighted avg       0.85      0.84      0.84      1960\n",
            "\n",
            "Test - Accuracy : 0.6052318668252081\n",
            "Test - Confusion matrix :\n",
            " [[ 86   4  33   8  17]\n",
            " [ 12 114  24  17   3]\n",
            " [ 17  13 125  17  11]\n",
            " [ 14  19  35 106  11]\n",
            " [ 24   3  31  19  78]]\n",
            "Test - classification report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.58      0.57       148\n",
            "           1       0.75      0.67      0.71       170\n",
            "           2       0.50      0.68      0.58       183\n",
            "           3       0.63      0.57      0.60       185\n",
            "           4       0.65      0.50      0.57       155\n",
            "\n",
            "    accuracy                           0.61       841\n",
            "   macro avg       0.62      0.60      0.61       841\n",
            "weighted avg       0.62      0.61      0.61       841\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGzMmmUblNOc",
        "colab_type": "code",
        "outputId": "41b227ac-ed15-4a4a-9416-68dfc706dd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Now let's look at one example.   Choose a test example by setting tx = value\n",
        "# Try 0, 1801, 531, 1500, 99, 777\n",
        "tx = 0\n",
        "\n",
        "print(\"newsgroups_test example number\", tx, \":\")\n",
        "print(newsgroups_test.data[tx])\n",
        "#print(X_test.shape)\n",
        "\n",
        "print(\"\\nThe associated TFIDF vector:\")\n",
        "print(X_test[tx])\n",
        "\n",
        "print(\"\\nThe model classifies this example as:\")\n",
        "y_test_example = clf.predict(X_test[tx])\n",
        "print(\"Category = \", y_test_example, \"=\", categories[int(y_test_example)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "newsgroups_test example number 0 :\n",
            "\n",
            "\n",
            "\"This is your god\" (from John Carpenter's \"They Live,\" natch)\n",
            "\n",
            "\n",
            "\n",
            "The associated TFIDF vector:\n",
            "  (0, 29912)\t0.26750324743587045\n",
            "  (0, 29918)\t0.31543082231244873\n",
            "  (0, 31375)\t0.26239467065053323\n",
            "  (0, 98408)\t0.16463577011982283\n",
            "  (0, 98483)\t0.31543082231244873\n",
            "  (0, 120184)\t0.1610908752373777\n",
            "  (0, 120191)\t0.29991874236005844\n",
            "  (0, 152013)\t0.17626588255558212\n",
            "  (0, 152042)\t0.288912746481491\n",
            "  (0, 184984)\t0.12335578536189261\n",
            "  (0, 185233)\t0.31543082231244873\n",
            "  (0, 195494)\t0.21057314470311617\n",
            "  (0, 195510)\t0.31543082231244873\n",
            "  (0, 196318)\t0.2335572485629079\n",
            "  (0, 196319)\t0.29991874236005844\n",
            "\n",
            "The model classifies this example as:\n",
            "Category =  [2] = rec.motorcycles\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}