{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Document_Classification_NB.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"BRtteY3D86Pq","colab_type":"text"},"source":["### Document Classification\n","\n","We can  represent unstructured text as a vector of features each of which have an associated frequency count.  This allows us to to develop classification models using machine learning algorithms. Let’s use a subset newsgroups text to build a classification model and assess its accuracy."]},{"cell_type":"code","metadata":{"id":"I5yzPZDK86Pt","colab_type":"code","colab":{}},"source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import Normalizer\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans, MiniBatchKMeans\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P4SSFb5M86Py","colab_type":"text"},"source":["### Load Data\n","Download data from 20 news groups "]},{"cell_type":"code","metadata":{"id":"lO8_RaSW86Pz","colab_type":"code","outputId":"8fd9d085-3a2d-4c45-c346-1622263c426e","executionInfo":{"status":"ok","timestamp":1584746142324,"user_tz":180,"elapsed":868,"user":{"displayName":"Danny Silver","photoUrl":"","userId":"16434078982045343492"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["newsgroups_train = fetch_20newsgroups(subset='train')\n","print(list(newsgroups_train.target_names))\n","\n","newsgroups_test = fetch_20newsgroups(subset='train')"],"execution_count":48,"outputs":[{"output_type":"stream","text":["['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q23Vn8xQ86P4","colab_type":"text"},"source":["##Prepare the Data\n","To keep it simple, let's filter only 5 of the 20 topics. \n","We will then convert the unstructured text to a structured vector of thousands of features made up of the words from the documents.  Stop words like “is”, “the”, “it” wil be removed.  Please look up   Each feature has a TFIDF value tha can be used calculate probabilities. Look up the SKLearn's TfidfVectorizer function to see ways that you may improve the data preparation - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html "]},{"cell_type":"code","metadata":{"id":"gUcoc06Q86P4","colab_type":"code","outputId":"0a230321-af15-4aa6-9bd1-24834bac6ca0","executionInfo":{"status":"ok","timestamp":1584746145346,"user_tz":180,"elapsed":3881,"user":{"displayName":"Danny Silver","photoUrl":"","userId":"16434078982045343492"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["#Categories     0               1                   2               3             4\n","categories = ['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'sci.space', 'talk.politics.guns']\n","\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, \n","                                      shuffle=True, random_state=2017, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, \n","                                     shuffle=True, random_state=2017, remove=('headers', 'footers', 'quotes'))\n","\n","y_train = newsgroups_train.target\n","y_test = newsgroups_test.target\n","\n","# Convert a collection of raw documents to a matrix of TF-IDF features\n","vectorizer = TfidfVectorizer()    # This one is the basic text to feature vector function, try some of the options\n","#vectorizer = TfidfVectorizer(lowercase=False, stop_words='english')\n","#vectorizer = TfidfVectorizer(smooth_idf = True, max_df=0.5, stop_words='english')\n","#vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df=0.5,  ngram_range=(1, 2), stop_words='english')\n","X_train = vectorizer.fit_transform(newsgroups_train.data)  # Learn vocabulary and idf, return term-document matrix.\n","X_test = vectorizer.transform(newsgroups_test.data)        # Transform documents to term-document matrix.\n","\n","print(\"Train Dataset\")\n","print(\"%d documents\" % len(newsgroups_train.data))\n","print(\"%d categories\" % len(newsgroups_train.target_names))\n","print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n","\n","print(\"\\nTest Dataset\")\n","print(\"%d documents\" % len(newsgroups_test.data))\n","print(\"%d categories\" % len(newsgroups_test.target_names))\n","print(\"n_samples: %d, n_features: %d\" % X_test.shape)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Train Dataset\n","2801 documents\n","5 categories\n","n_samples: 2801, n_features: 33922\n","\n","Test Dataset\n","1864 documents\n","5 categories\n","n_samples: 1864, n_features: 33922\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F6fiVUeD86P7","colab_type":"text"},"source":["### Naive Bayes Model\n","Lets build a simple Naive Bayes classifier and assess its performance on the train and independent test set.  This classifier can be replaced by any other SKLearn classification ML algorithm."]},{"cell_type":"code","metadata":{"id":"8H27weer86P8","colab_type":"code","outputId":"eb362c2a-2264-4e42-930b-f0ec0ee52426","executionInfo":{"status":"ok","timestamp":1584746145346,"user_tz":180,"elapsed":3875,"user":{"displayName":"Danny Silver","photoUrl":"","userId":"16434078982045343492"}},"colab":{"base_uri":"https://localhost:8080/","height":535}},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn import metrics\n","\n","clf = MultinomialNB()\n","clf = clf.fit(X_train, y_train)\n","\n","y_train_pred = clf.predict(X_train)\n","y_test_pred = clf.predict(X_test)\n","\n","print(\"Categories: 0=alt.atheism, 1=comp.graphics, 2=rec.motorcycles, 3=sci.space, 4=talk.politics.guns\\n\")\n","print('Train accuracy_score: ', metrics.accuracy_score(y_train, y_train_pred))\n","print('Test accuracy_score: ',metrics.accuracy_score(newsgroups_test.target, y_test_pred))\n","\n","print(\"Train Metrics: \")\n","print(metrics.classification_report(y_train, y_train_pred))\n","print(\"Test Metrics: \")\n","print(metrics.classification_report(newsgroups_test.target, y_test_pred))"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Categories: 0=alt.atheism, 1=comp.graphics, 2=rec.motorcycles, 3=sci.space, 4=talk.politics.guns\n","\n","Train accuracy_score:  0.9532309889325241\n","Test accuracy_score:  0.8170600858369099\n","Train Metrics: \n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.91      0.95       480\n","           1       0.98      0.95      0.97       584\n","           2       0.89      0.97      0.93       598\n","           3       0.98      0.95      0.97       593\n","           4       0.93      0.97      0.95       546\n","\n","    accuracy                           0.95      2801\n","   macro avg       0.96      0.95      0.95      2801\n","weighted avg       0.96      0.95      0.95      2801\n","\n","Test Metrics: \n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.61      0.73       319\n","           1       0.93      0.86      0.89       389\n","           2       0.80      0.88      0.84       398\n","           3       0.81      0.82      0.82       394\n","           4       0.71      0.88      0.79       364\n","\n","    accuracy                           0.82      1864\n","   macro avg       0.83      0.81      0.81      1864\n","weighted avg       0.83      0.82      0.82      1864\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xyjLOjNZKFsI","colab_type":"code","outputId":"58cc3808-202b-459c-a071-df7371d0ca63","executionInfo":{"status":"ok","timestamp":1584746145347,"user_tz":180,"elapsed":3870,"user":{"displayName":"Danny Silver","photoUrl":"","userId":"16434078982045343492"}},"colab":{"base_uri":"https://localhost:8080/","height":363}},"source":["# Now let's look at one example.   Choose a test example by setting tx = value\n","# Try 0, 1801, 531, 1500, 99, 777\n","tx = 0\n","\n","print(\"newsgroups_test example number\", tx, \":\")\n","print(newsgroups_test.data[tx])\n","#print(X_test.shape)\n","\n","print(\"\\nThe associated TFIDF vector:\")\n","print(X_test[tx])\n","\n","print(\"\\nThe model classifies this example as:\")\n","y_test_example = clf.predict(X_test[tx])\n","print(\"Category = \", y_test_example, \"=\", categories[int(y_test_example)])"],"execution_count":51,"outputs":[{"output_type":"stream","text":["newsgroups_test example number 0 :\n","\n","\n","\"This is your god\" (from John Carpenter's \"They Live,\" natch)\n","\n","\n","\n","The associated TFIDF vector:\n","  (0, 33676)\t0.21278244729242746\n","  (0, 30245)\t0.14875616353796775\n","  (0, 30205)\t0.18044341955917026\n","  (0, 18944)\t0.354578989615599\n","  (0, 17558)\t0.39729413412566733\n","  (0, 17187)\t0.1242731377217399\n","  (0, 14435)\t0.3338754652783908\n","  (0, 13788)\t0.1794773057559302\n","  (0, 7441)\t0.6762675321413448\n","\n","The model classifies this example as:\n","Category =  [0] = alt.atheism\n"],"name":"stdout"}]}]}